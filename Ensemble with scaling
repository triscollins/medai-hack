# =========================================================
# 1. Import Libraries & Load Data
# =========================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# For our Deep Neural Network
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

#For splitting the training and testing data
from sklearn.model_selection import train_test_split 

# Some display settings for nicer graphs
plt.rcParams['figure.figsize'] = (10, 6)
sns.set_style('whitegrid')

# Load dataset
dataset_df = pd.read_csv('/kaggle/input/medhack-ai-hospital-datasets/train_data.csv')
print("Data Loaded! Shape:", dataset_df.shape)
print(dataset_df.head())

# Load submission data
submission_df = pd.read_csv('/kaggle/input/medhack-ai-hospital-datasets/test_data.csv')
print("Submission Data Loaded! Shape:", submission_df.shape)
print(submission_df.head())


# 2.1: Drop columns we don't want or need from training data
# They might not directly help us predict in a simple DNN approach.
dataset_df = dataset_df.drop([
    'timestamp', 'patient_id', 'first_name', 'last_name', 
    'age', 'address', 'gender', 'city', 'state', 'postcode'
], axis=1)


submission_df = submission_df.drop([
    'timestamp', 'patient_id', 'first_name', 'last_name', 
    'age', 'address', 'gender', 'city', 'state', 'postcode'
], axis=1)



# =========================================================
# 2. Preprocessing
# =========================================================


# 2.1: Check for missing values
print("Missing values in training data:\n", dataset_df.isna().sum())

# Example strategy: just drop rows with missing data.
# (Real-world might do more nuanced imputation.)
dataset_df = dataset_df.dropna()
submission_df = submission_df.dropna()


# 2.2: Extract features (X) and labels (y) from the training set
X_all = dataset_df.drop('state_label', axis=1)
y_all = dataset_df['state_label']

# Convert labels {2,3} -> 1 to make it binary
#y_all = y_all.replace({2: 1, 3: 1})

# Perform the split (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X_all, 
    y_all,
    test_size=0.2,     # Size of the test set (0.25 = 25% of data)
    random_state=42,     # Set seed for reproducibility
    shuffle=True         # Shuffle the data before splitting
)

#2.4 Print shape of training set
print("Training Features shape:", X_train.shape)
print("Training Labels shape:", y_train.shape)


#2.4 Print shape of testing set
print("Training Features shape:", X_test.shape)
print("Training Labels shape:", y_test.shape)

from sklearn.preprocessing import StandardScaler

submission_df_predict = submission_df.drop(['ID'], axis=1)

# Initialize the scaler
scaler = MinMaxScaler()

# Fit the scaler on X_train
scaler.fit(X_train)

# Transform both X_train and X_test
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
submission_scaled = scaler.transform(submission_df_predict)


# =========================================================
# 3. Ensemble model
# =========================================================

# Ensemble method
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Create base models
knn_model = KNeighborsClassifier(
   n_neighbors=5,
   weights='distance'  # Weight points by distance
)

rf_model = RandomForestClassifier(
   class_weight='balanced',  # For imbalanced dataset
   n_estimators=10,
   random_state=42
)

# Create voting classifier
ensemble = VotingClassifier(
   estimators=[
       ('knn', knn_model),
       ('rf', rf_model)
   ],
   voting='soft'  # Use probability predictions
)

# Fit the ensemble
ensemble.fit(X_train_scaled, y_train)

# Make predictions
ensemble_predictions = ensemble.predict(X_test_scaled)

# Print overall accuracy
print("Ensemble Model Accuracy:", accuracy_score(y_test, ensemble_predictions))


# =========================================================
# 4. Predictions on submission dataset
# =========================================================

# Make predictions on 
submission_predictions = ensemble.predict(submission_scaled)

# Create a DataFrame from the predicted labels
predictions = pd.DataFrame(submission_predictions, columns=['predicted_label'])

predictions_df = pd.DataFrame({
    'ID': submission_df['ID'],  # Keep the same ID from test data
    'predicted_label': predictions['predicted_label']
})

# Save DataFrame to csv
predictions_df.to_csv('predictions.csv', index=False)

from IPython.display import FileLink
FileLink(r'predictions.csv')
