# =========================================================
# 1. Import Libraries & Load Data
# =========================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# For our Deep Neural Network
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

#For splitting the training and testing data
from sklearn.model_selection import train_test_split 

# Some display settings for nicer graphs
plt.rcParams['figure.figsize'] = (10, 6)
sns.set_style('whitegrid')

# Load dataset
dataset_df = pd.read_csv('/kaggle/input/medhack-ai-hospital-datasets/train_data.csv')
print("Data Loaded! Shape:", dataset_df.shape)
print(dataset_df.head())

# Load submission data
submission_df = pd.read_csv('/kaggle/input/medhack-ai-hospital-datasets/test_data.csv')
print("Submission Data Loaded! Shape:", submission_df.shape)
print(submission_df.head())


# 2.1: Drop columns we don't want or need from training data
# They might not directly help us predict in a simple DNN approach.
dataset_df = dataset_df.drop([
    'timestamp', 'patient_id', 'first_name', 'last_name', 
    'age', 'address', 'gender', 'city', 'state', 'postcode'
], axis=1)


submission_df = submission_df.drop([
    'timestamp', 'patient_id', 'first_name', 'last_name', 
    'age', 'address', 'gender', 'city', 'state', 'postcode'
], axis=1)



# =========================================================
# 2. Preprocessing
# =========================================================


# 2.1: Check for missing values
print("Missing values in training data:\n", dataset_df.isna().sum())

# Example strategy: just drop rows with missing data.
# (Real-world might do more nuanced imputation.)
dataset_df = dataset_df.dropna()
submission_df = submission_df.dropna()


# 2.2: Extract features (X) and labels (y) from the training set
X_all = dataset_df.drop('state_label', axis=1)
y_all = dataset_df['state_label']

# Convert labels {2,3} -> 1 to make it binary
#y_all = y_all.replace({2: 1, 3: 1})

# Perform the split (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X_all, 
    y_all,
    test_size=0.2,     # Size of the test set (0.25 = 25% of data)
    random_state=42,     # Set seed for reproducibility
    shuffle=True         # Shuffle the data before splitting
)

#2.4 Print shape of training set
print("Training Features shape:", X_train.shape)
print("Training Labels shape:", y_train.shape)


#2.4 Print shape of testing set
print("Training Features shape:", X_test.shape)
print("Training Labels shape:", y_test.shape)

from sklearn.preprocessing import StandardScaler

submission_df_predict = submission_df.drop(['ID'], axis=1)

# Initialize the scaler
scaler = MinMaxScaler()

# Fit the scaler on X_train
scaler.fit(X_train)

# Transform both X_train and X_test
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
submission_scaled = scaler.transform(submission_df_predict)



# =========================================================
# 3. Ensemble model with penalties
# =========================================================

from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, make_scorer
from sklearn.utils.class_weight import compute_class_weight
import numpy as np
import pandas as pd
from IPython.display import FileLink

def weighted_accuracy_score(y_true, y_pred, class_penalties):
 """
 Custom scoring function that applies penalties for incorrect predictions
 based on class importance
 """
 correct = (y_true == y_pred)
 weights = np.array([class_penalties[label] for label in y_true])
 return np.sum(correct * weights) / np.sum(weights)

# Compute initial class weights based on frequency
unique_classes = np.unique(y_train)
class_weights = compute_class_weight(
 class_weight='balanced',
 classes=unique_classes,
 y=y_train
)

# Modify class weights to increase penalties for classes 1, 2, and 3
# Create a custom class weight dictionary
class_weight_dict = dict(zip(unique_classes, class_weights))
# Increase weights for classes 1, 2, and 3 by multiplying them by 2
for class_label in [1, 2, 3]:
 if class_label in class_weight_dict:
 class_weight_dict[class_label] *= 5

# Create penalty dictionary using the modified weights
class_penalties = class_weight_dict

# Create custom scorer
weighted_scorer = make_scorer(
 weighted_accuracy_score,
 class_penalties=class_penalties
)

# Create base models
knn_model = KNeighborsClassifier(
 n_neighbors=5,
 weights='distance' # Weight points by distance - this helps with class imbalance
)

# Modify Random Forest to use the custom class weights
rf_model = RandomForestClassifier(
 class_weight=class_weight_dict, # Use our custom class weights
 n_estimators=10,
 random_state=42
)

# Create voting classifier with adjusted weights for RF vs KNN
ensemble = VotingClassifier(
 estimators=[
 ('knn', knn_model),
 ('rf', rf_model)
 ],
 weights=[0.3, 0.7], # Give even more weight to RF since it now handles class penalties better
 voting='soft' # Use probability predictions
)

# Fit the models separately
# KNN without sample weights
knn_model.fit(X_train_scaled, y_train)

# RF with modified sample weights
sample_weights = np.array([class_penalties[label] for label in y_train])
rf_model.fit(X_train_scaled, y_train, sample_weight=sample_weights)

# Fit the ensemble
ensemble.fit(X_train_scaled, y_train)

# Make predictions on test set
ensemble_predictions = ensemble.predict(X_test_scaled)

# Print model performance metrics
print("Model Performance on Test Set:")
print("==============================")
print("Standard Accuracy:", accuracy_score(y_test, ensemble_predictions))
print("Weighted Accuracy:", weighted_accuracy_score(y_test, ensemble_predictions, class_penalties))

# Print class-specific performance
print("\nClass-specific Performance:")
print("==========================")
for class_label in unique_classes:
 mask = y_test == class_label
 class_acc = accuracy_score(y_test[mask], ensemble_predictions[mask])
 print(f"Class {class_label} Accuracy: {class_acc:.3f}")


# =========================================================
# 4. Predictions on submission dataset
# =========================================================

# Make predictions using the modified ensemble
submission_predictions = ensemble.predict(submission_scaled)

# Create submission DataFrame
predictions_df = pd.DataFrame({
    'ID': submission_df['ID'],
    'predicted_label': submission_predictions
})

# Save predictions to CSV
output_file = 'predictions_weighted_ensemble.csv'
predictions_df.to_csv(output_file, index=False)

# Display download link
FileLink(output_file)

# Print prediction distribution to check for class imbalance
print("\nPrediction Distribution in Submission Set:")
print("=========================================")
print(pd.Series(submission_predictions).value_counts(normalize=True))
